# Similarity in RAG (Retrievalâ€‘Augmented Generation)

---

## Good question â€” this is **one of the most important foundations of RAG**.

If you truly understand **similarity**, you understand how retrieval actually works.

Letâ€™s go stepâ€‘byâ€‘step from intuition â†’ math â†’ types â†’ when to use â†’ MMR mapping.

---

# Similarity in RAG (Retrieval-Augmented Generation)

## 1. What is â€œSimilarityâ€ in RAG?

In **RAG**, similarity answers one core question:

> **Which stored document chunks are most related to the user query?**

Since LLMs cannot search raw text efficiently, we convert text into **vectors (embeddings)**.

```
Text â†’ Embedding Model â†’ Vector (numbers)
```

Example:

```
Query: "What is ANN search?"

â†’ [0.21, -0.44, 0.89, ...]
```

Every document chunk also becomes a vector.

Retrieval = **compare vectors and find closest ones**.

So similarity = **mathematical closeness between vectors**.

---

## 2. Why Similarity Matters in RAG

RAG pipeline:

```
User Query
     â†“
Embedding Model
     â†“
Similarity Search   â† (CORE PART)
     â†“
Top-k Documents
     â†“
LLM Answer
```

If similarity is wrong â†’ retrieval is wrong â†’ hallucination increases.

---

# 3. Vector Similarity â€” Intuition

Imagine vectors as **points in space**.

Two texts are similar if:

* They point in same direction
* They are close in space
* Their semantic meaning overlaps

Different similarity metrics measure this differently.

---

# 4. Main Types of Similarity Metrics

We divide them into **two families**:

| Family     | Measures        |
| ---------- | --------------- |
| Similarity | Higher = better |
| Distance   | Lower = better  |

---

# A. COSINE SIMILARITY (Most Important)

## Idea

Measures **angle between vectors**, not magnitude.

```
Same direction â†’ high similarity
Different direction â†’ low similarity
```

### Formula

```
cos(Î¸) = (A Â· B) / (||A|| ||B||)
```

Range:

```
-1 â†’ opposite
0  â†’ unrelated
1  â†’ identical meaning
```

---

## Why Cosine Works Well for NLP

Embeddings encode **semantic direction**, not length.

Example:

```
"dog animal"
"puppy pet"
```

Vectors point similarly â†’ high cosine similarity.

---

## Advantages

* Scale independent
* Stable embeddings
* Best semantic matching
* Default for most vector DBs

---

## Used In

* FAISS
* Pinecone
* Chroma
* Weaviate
* OpenAI embeddings
* Sentence Transformers

---

## Use When

âœ… Semantic search
âœ… RAG retrieval
âœ… Question answering
âœ… General NLP

ğŸ‘‰ **Default choice for RAG**

---

# B. DOT PRODUCT (Inner Product)

## Idea

Measures alignment **and magnitude**.

```
A Â· B = Î£ AiBi
```

Large vectors produce larger scores.

---

## Intuition

```
Cosine â†’ direction only
Dot Product â†’ direction + strength
```

---

## When Useful

If embedding magnitude carries meaning.

Some models intentionally encode confidence in vector length.

---

## Used In

* Maximum Inner Product Search (MIPS)
* Dense Passage Retrieval (DPR)
* Large-scale recommender systems

---

## Use When

âœ… Model trained with dot-product objective
âœ… Recommendation systems
âœ… DPR-style retrieval

âŒ Not ideal if vectors not normalized.

---

# C. DISTANCE METRICS

Distance = **how far apart vectors are**.

Lower distance â†’ higher similarity.

---

## 1. Euclidean Distance (L2)

### Formula

```
sqrt(Î£ (Ai âˆ’ Bi)^2)
```

Straight-line distance.

### Intuition

Geometric closeness.

---

### Pros

* Natural geometric measure
* Works in low dimensions

### Cons

* High-dimensional embeddings suffer (curse of dimensionality)

---

### Use When

âœ… Image embeddings
âœ… Physical measurements
âš ï¸ Less common in text RAG

---

## 2. Manhattan Distance (L1)

```
Î£ |Ai âˆ’ Bi|
```

Grid-like movement distance.

---

### Use When

* Sparse vectors
* Feature-based ML
* Some classical IR systems

Rare in modern RAG.

---

## 3. Hamming Distance

Counts number of different bits.

```
10101
11100
â†“
differences = 3
```

---

### Use When

âœ… Binary embeddings
âœ… Hash-based retrieval
âœ… ANN indexing tricks

Not used for normal text embeddings.

---

# 5. Quick Comparison Table

| Metric      | Measures          | Best For            | RAG Usage |
| ----------- | ----------------- | ------------------- | --------- |
| Cosine      | Angle             | Semantic similarity | â­â­â­â­â­     |
| Dot Product | Angle + magnitude | DPR, recommenders   | â­â­â­â­      |
| Euclidean   | Spatial distance  | vision/audio        | â­â­        |
| Manhattan   | Axis distance     | sparse data         | â­         |
| Hamming     | Bit difference    | binary vectors      | â­         |

---

# 6. How Vector Databases Map Them

Important insight:

> Many systems internally convert metrics.

Example:

If vectors are **normalized**:

```
Cosine similarity â‰ˆ Dot product
```

Because:

```
||A|| = 1
||B|| = 1
```

So:

```
A Â· B = cosine similarity
```

Thatâ€™s why many ANN libraries use inner product internally.

---

# 7. ANN Search + Similarity

Approximate Nearest Neighbor (ANN) indexes optimize search based on metric.

| ANN Index | Works Best With |
| --------- | --------------- |
| HNSW      | Cosine / Dot    |
| IVF       | Euclidean       |
| PQ        | Euclidean       |
| ScaNN     | Dot/Cosine      |

Choosing wrong metric reduces recall.

---

# 8. MMR (Maximal Marginal Relevance)

Now comes the **advanced part**.

---

## Problem: Similarity Collapse

Top-k retrieval often returns:

```
Doc 1: ANN explanation
Doc 2: ANN explanation (same)
Doc 3: ANN explanation (same)
```

All similar â†’ low diversity.

LLM receives redundant context.

---

## MMR Idea

Balance:

```
Relevance  +  Diversity
```

Instead of:

```
Most similar documents
```

We choose:

```
Relevant BUT different documents
```

---

## MMR Formula (Conceptual)

```
MMR = Î» Ã— similarity(query, doc)
      âˆ’ (1 âˆ’ Î») Ã— similarity(doc, selected_docs)
```

Where:

* First term â†’ relevance
* Second term â†’ redundancy penalty

---

## Lambda Parameter

| Î»   | Behavior               |
| --- | ---------------------- |
| 1.0 | Pure similarity        |
| 0.7 | Balanced (recommended) |
| 0.3 | More diversity         |

---

## MMR Retrieval Flow

```
1. Retrieve many candidates (top 20â€“50)
2. Pick most relevant
3. Penalize similar docs
4. Select diverse set
```

---

## Why MMR is Powerful in RAG

LLMs need **coverage**, not duplicates.

MMR gives:

* broader context
* fewer hallucinations
* better answers

---

# 9. Mapping â€” What to Use Where (VERY IMPORTANT)

## Practical Decision Guide

| Scenario              | Similarity  | MMR?       | Why                     |
| --------------------- | ----------- | ---------- | ----------------------- |
| Standard RAG          | Cosine      | âœ… Yes      | Best semantic retrieval |
| QA chatbot            | Cosine      | âœ… Yes      | Avoid duplicates        |
| Research assistant    | Cosine      | âœ… Strongly | Needs coverage          |
| Recommendation system | Dot Product | Optional   | magnitude useful        |
| Image search          | Euclidean   | Optional   | geometric space         |
| Binary ANN index      | Hamming     | No         | hash matching           |

---

## Industry Default Stack

```
Embedding model â†’ cosine similarity
ANN index â†’ HNSW
Retriever â†’ Top-k
Post-process â†’ MMR
LLM â†’ Generation
```

This is used in many production RAG systems.

---

# 10. LangChain Example

### Cosine Similarity Retrieval

```python
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)
```

---

### MMR Retrieval

```python
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,
        "fetch_k": 20,
        "lambda_mult": 0.7
    }
)
```

---

# 11. Mental Model (Remember Forever)

Think:

```
Cosine â†’ Find relevant docs
MMR â†’ Find useful set of docs
```

Similarity answers:

> â€œWhich documents match the query?â€

MMR answers:

> â€œWhich combination of documents helps the LLM most?â€

---

# 12. Final Cheat Sheet

```
IF building RAG â†’ use COSINE
IF duplicates appear â†’ add MMR
IF using DPR â†’ DOT PRODUCT
IF image/vector geometry â†’ EUCLIDEAN
IF binary index â†’ HAMMING
```

---

## One-Line Summary

**Similarity metrics decide WHAT you retrieve.
MMR decides WHICH COMBINATION you send to the LLM.**
