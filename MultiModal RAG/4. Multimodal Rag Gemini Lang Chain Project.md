# Multimodal RAG using Gemini + LangChain

---

## ğŸš€ Project Overview

This project demonstrates how to build a **Multimodal Retrieval-Augmented Generation (Multimodal RAG)** system using:

* **Google Gemini (Text + Vision Models)**
* **LangChain Framework**
* **FAISS Vector Database**
* **Image + Text Understanding**

The system accepts an **image input**, extracts semantic meaning using a **Vision LLM**, retrieves relevant information from a **knowledge base**, and generates a **grounded final response**.

---

## ğŸ§  What is Multimodal RAG?

Multimodal RAG extends traditional RAG by allowing multiple data modalities such as:

| Modality             | Example                        |
| -------------------- | ------------------------------ |
| Text                 | Documents, PDFs                |
| Images               | Product photos                 |
| Vision Understanding | Object detection & recognition |

### Pipeline Concept

```
Image â†’ Vision Model â†’ Text Query â†’ Retriever â†’ Context â†’ LLM â†’ Final Answer
```

---

## ğŸ—ï¸ System Architecture

```
                IMAGE INPUT
                      â†“
              Gemini Vision Model
                      â†“
               Text Description
                      â†“
                 Retriever
                (FAISS DB)
                      â†“
              Relevant Context
                      â†“
               Gemini Text Model
                      â†“
                 Final Answer
```

---

## ğŸ“¦ Installation

Install all dependencies:

```bash
pip install --upgrade \
  langchain \
  langchain-google-genai \
  "langchain[docarray]" \
  faiss-cpu \
  pypdf
```

---

## ğŸ”‘ Environment Setup

Store your Google API key securely.

```python
from google.colab import userdata
import os

GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
```

---

## ğŸ¤– Model Loader

Loads either text or vision Gemini models.

```python
def load_model(model_name):
    if model_name == "gemini-pro":
        llm = ChatGoogleGenerativeAI(model="gemini-pro")
    else:
        llm = ChatGoogleGenerativeAI(model="gemini-pro-vision")
    return llm
```

| Model             | Purpose             |
| ----------------- | ------------------- |
| gemini-pro        | Text reasoning      |
| gemini-pro-vision | Image understanding |

---

## ğŸ–¼ï¸ Image Processing

Download and display images dynamically.

```python
def get_image(url, filename, extension):
    content = requests.get(url).content
    with open(f'/content/{filename}.{extension}', 'wb') as f:
        f.write(content)
    image = Image.open(f"/content/{filename}.{extension}")
    image.show()
    return image
```

---

## ğŸ‘ï¸ Vision Model Usage

Send text + image together to Gemini Vision.

```python
message = HumanMessage(
    content=[
        {"type": "text", "text": prompt},
        {"type": "image_url", "image_url": image}
    ]
)
```

The model produces a semantic description of the image.

---

## ğŸ“„ Loading Knowledge Base

Text knowledge is loaded using LangChain loaders.

```python
loader = TextLoader("/content/nike_shoes.txt")
text = loader.load()[0].page_content
```

---

## âœ‚ï¸ Text Chunking

Splits large text into smaller semantic chunks.

```python
def get_text_chunks_langchain(text):
    text_splitter = CharacterTextSplitter(
        chunk_size=20,
        chunk_overlap=10
    )
    docs = [Document(page_content=x) for x in text_splitter.split_text(text)]
    return docs
```

---

## ğŸ”¢ Embeddings Generation

Convert text into vector representations.

```python
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001"
)
```

---

## ğŸ§® Vector Database (FAISS)

Store embeddings for similarity search.

```python
vectorstore = FAISS.from_documents(docs, embedding=embeddings)
retriever = vectorstore.as_retriever()
```

Retriever performs semantic search:

```
Query â†’ Similar Documents
```

---

## ğŸ§© Prompt Template

Defines how retrieved context is injected.

```python
template = """
```

{context}

```

{query}

Provide brief information and store location.
"""
```

---

## ğŸ”— Text RAG Chain

```python
rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}
    | prompt
    | llm_text
    | StrOutputParser()
)
```

### Flow

```
User Query
   â†“
Retriever
   â†“
Context Injection
   â†“
Text LLM
   â†“
Final Answer
```

---

## ğŸŒ Multimodal RAG Chain

Vision output becomes input to RAG.

```python
full_chain = (
    RunnablePassthrough()
    | llm_vision
    | StrOutputParser()
    | rag_chain
)
```

### Multimodal Flow

```
Image
 â†“
Vision LLM
 â†“
Generated Text Query
 â†“
Retriever
 â†“
Knowledge Context
 â†“
Text LLM
 â†“
Grounded Response
```

---

## â–¶ï¸ Running the System

```python
result = full_chain.invoke([message])
```

Output contains:

* Product identification
* Context-aware explanation
* Store/location information

---

## ğŸ“Š Key Components Summary

| Component       | Role                  |
| --------------- | --------------------- |
| Gemini Vision   | Understand image      |
| Gemini Text     | Generate answers      |
| FAISS           | Vector search         |
| Embeddings      | Semantic encoding     |
| Retriever       | Fetch relevant chunks |
| Prompt Template | Context formatting    |
| RAG Chain       | Knowledge grounding   |

---

## âœ… Features

* Multimodal input (Image + Text)
* Retrieval-Augmented Generation
* Vision-to-Text conversion
* Semantic search using FAISS
* Context-grounded responses
* Modular LangChain pipeline

---

## ğŸ§ª Example Use Case

Input:

```
Nike sandal image
```

System performs:

1. Detect brand & model from image
2. Retrieve product information
3. Generate grounded explanation

Output:

```
Brand: Nike
Model: Calm Slides
Description + Store Location
```

---

## ğŸ“ Project Structure

```
project/
â”‚
â”œâ”€â”€ notebook.ipynb
â”œâ”€â”€ nike_shoes.txt
â”œâ”€â”€ README.md
â””â”€â”€ assets/
```

---

# Multimodal Embeddings & RAG â€” Project Explanation

---

## Part 1 â€” How YOU did embeddings in this project

Letâ€™s trace exactly what happens in your code.

---

### Step 1 â€” Text chunks are created

```python
docs = get_text_chunks_langchain(text)
```

Your Nike text file becomes:

* Chunk 1 â†’ "Nike sandals are designed..."
* Chunk 2 â†’ "Calm slides provide comfort..."
* Chunk 3 â†’ "Available in stores..."

Each chunk = **Document**.

---

### Step 2 â€” You created embeddings

```python
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001"
)
```

#### What this does

This model converts **text â†’ vector (numbers)**.

Example (conceptual):

```
"Nike sandal"
        â†“
[0.21, -0.44, 0.89, 0.10, ...]
```

This vector captures **semantic meaning**, not keywords.

---

### Step 3 â€” Store embeddings in FAISS

```python
vectorstore = FAISS.from_documents(docs, embedding=embeddings)
```

Internally:

```
Text Chunk â†’ Embedding Vector â†’ Stored in FAISS
```

So FAISS now contains:

| Text Chunk       | Vector            |
| ---------------- | ----------------- |
| Nike slides info | [0.12, 0.88, ...] |
| Comfort details  | [0.44, 0.02, ...] |
| Store locations  | [0.91, 0.33, ...] |

---

### Step 4 â€” Query embedding (VERY IMPORTANT)

When user asks:

```python
retriever.invoke("Nike slide/sandal")
```

LangChain automatically does:

```
User Query
   â†“
Embedding Model
   â†“
Query Vector
```

Example:

```
"Nike sandal"
â†’ [0.20, -0.41, 0.85, ...]
```

---

### Step 5 â€” Similarity Search

FAISS compares vectors using similarity:

```
Query Vector
      vs
Stored Vectors
```

Closest vectors = most relevant documents.

Returned chunks become:

```
{context}
```

inside your prompt.

---

## âœ… Summary â€” Your Embedding Pipeline

```
Text File
   â†“
Chunking
   â†“
Gemini Embedding Model
   â†“
Vectors
   â†“
FAISS Database
   â†“
Similarity Search
```

---

## Important Observation

You embedded **ONLY TEXT**.

Images were **NOT embedded**.

---

## VERY IMPORTANT INSIGHT

Your system is:

```
Vision â†’ Text â†’ Text Embedding RAG
```

NOT true multimodal embeddings.

You converted **image â†’ text first**.

---

# Part 2 â€” What are Multimodal Embeddings (CLIP)?

---

## Now the advanced concept.

### Problem with your current approach

Your pipeline:

```
Image
 â†“
Vision LLM describes image in words
 â†“
Text embedding search
```

#### Problem:

* Information may be lost
* Depends on LLM description quality
* Two-step reasoning

---

## Multimodal Embeddings solve this

Instead of converting image â†’ text,

we embed **BOTH directly into the same vector space**.

---

## CLIP (Contrastive Languageâ€“Image Pretraining)

CLIP is a model by OpenAI that learns:

```
Image â†” Text meaning alignment
```

It creates embeddings where:

* Image of Nike sandal
* Text "Nike sandal"

produce similar vectors.

---

## Conceptual Example

CLIP generates:

```
Image â†’ [0.45, 0.12, -0.77, ...]
Text  â†’ [0.44, 0.10, -0.75, ...]
```

Vectors are close â†’ semantic match.

---

## Shared Embedding Space

```
                VECTOR SPACE

        "dog" text     ğŸ• image
             â—-----------â—

        "car" text     ğŸš— image
             â—-----------â—
```

Both modalities live together.

---

## Multimodal Retrieval Flow (CLIP-based)

```
Image Query
     â†“
Image Embedding
     â†“
Vector DB Search
     â†“
Retrieve text OR images directly
```

No description step needed.

---

## Comparison â€” Your Method vs CLIP

| Feature          | Your System     | CLIP Multimodal  |
| ---------------- | --------------- | ---------------- |
| Image handling   | Convert to text | Direct embedding |
| Embeddings       | Text only       | Image + Text     |
| Vector space     | Text space      | Shared space     |
| Accuracy         | Medium          | High             |
| Speed            | Slower          | Faster           |
| Information loss | Possible        | Minimal          |

---

## Visual Comparison

### Your Pipeline (Option 2 Multimodal RAG)

```
Image
 â†“
Vision LLM
 â†“
Text Description
 â†“
Text Embeddings
 â†“
Retrieval
```

---

### CLIP Pipeline (True Multimodal)

```
Image â”€â”€â”
         â”œâ”€â”€ CLIP Encoder â†’ Same Vector Space
Text  â”€â”€â”˜
```

---

## Why Industry Moves Toward CLIP

Because it enables:

* Image search engines
* Product matching
* Visual recommendation systems
* Cross-modal retrieval
* Video understanding

Used by:

* Pinterest
* Google Lens (similar idea)
* E-commerce search
* Multimodal RAG systems

---

## Key Takeaway (Interview-Level Answer)

You can say:

> In my project, embeddings were created using Gemini text embeddings for document chunks. Images were first converted into text using a vision model, and retrieval was performed in text embedding space. True multimodal embeddings like CLIP instead embed images and text into a shared vector space, enabling direct cross-modal retrieval without converting images into text.

---



---

## ğŸ”® Future Improvements

* Multimodal embeddings (CLIP)
* Hybrid search (BM25 + Vector)
* Reranking models
* Streaming responses
* Production API deployment
* LangGraph agent workflow

---

## ğŸ§‘â€ğŸ’» Technologies Used

* LangChain
* Google Gemini API
* FAISS
* Python
* PIL
* Matplotlib


---

## â­ Acknowledgements

* Google Gemini
* LangChain
* FAISS
