# FlashRank Reranking + LangChain Retrieval Pipeline

---

## ğŸ“Œ Overview

This project demonstrates **FlashRank-based reranking** integrated with a **LangChain Retrieval Pipeline** using:

* FlashRank Crossâ€‘Encoder reranking
* FAISS vector database
* OpenAI embeddings
* Contextual Compression Retriever
* RetrievalQA chain

The notebook shows how to:

1. Perform **passage reranking** using FlashRank
2. Build a **vector store retriever**
3. Apply **reranking as contextual compression**
4. Improve Retrieval-Augmented Generation (RAG) answers

---

# ğŸš€ Model Options (FlashRank)

FlashRank provides multiple lightweight reranking models.

| Model  | Size   | Speed     | Performance               | Use Case         |
| ------ | ------ | --------- | ------------------------- | ---------------- |
| Nano   | ~4MB   | âš¡ Fastest | Competitive               | Low-latency apps |
| Small  | ~34MB  | Fast      | Best ranking precision    | Balanced usage   |
| Medium | ~110MB | Slower    | Best zero-shot ranking    | Research         |
| Large  | ~150MB | Slow      | Multilingual (100+ langs) | Global search    |

---

## âš¡ FlashRank Characteristics

* Ultraâ€‘lite CPU execution
* No heavy dependencies
* Serverless friendly
* Crossâ€‘encoder based reranking
* Optimized for retrieval pipelines

Supported internal models include:

* `ms-marco-TinyBERT-L-2-v2` (default)
* `ms-marco-MiniLM-L-12-v2`
* `rank-T5-flan`
* `ms-marco-MultiBERT-L-12`

---

# ğŸ§© Step 1 â€” Install FlashRank

```python
!pip install flashrank
```

Installs the FlashRank library used for reranking passages.

---

# ğŸ§© Step 2 â€” Helper Function (Document Printing)

```python
def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [
                f"Document {i+1}:\n\n{d.page_content}\nMetadata: {d.metadata}"
                for i, d in enumerate(docs)
            ]
        )
    )
```

### Purpose

* Nicely formats retrieved documents
* Shows:

  * document content
  * metadata

Useful for debugging retrieval pipelines.

---

# ğŸ§© Step 3 â€” Define Query

```python
query = "How to speedup LLMs?"
```

This query will be used for passage ranking.

---

# ğŸ§© Step 4 â€” Define Passages

```python
passages = [
   {
      "id":1,
      "text":"Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.",
      "meta": {"additional": "info1"}
   },
   {
      "id":2,
      "text":"LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper",
      "meta": {"additional": "info2"}
   },
   {
      "id":3,
      "text":"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint...",
      "meta": {"additional": "info3"}
   },
   {
      "id":4,
      "text":"Ever want to make your LLM inference go brrrrr... Medusa framework... 2x speedup.",
      "meta": {"additional": "info4"}
   },
   {
      "id":5,
      "text":"vLLM is a fast and easy-to-use library for LLM inference and serving...",
      "meta": {"additional": "info5"}
   }
]
```

### Structure

Each passage contains:

| Field | Meaning             |
| ----- | ------------------- |
| id    | Unique identifier   |
| text  | Passage content     |
| meta  | Additional metadata |

---

# ğŸ§© Step 5 â€” Import FlashRank

```python
from flashrank.Ranker import Ranker, RerankRequest
```

### Components

* **Ranker** â†’ Crossâ€‘encoder scoring model
* **RerankRequest** â†’ Input structure for reranking

---

# ğŸ§© Step 6 â€” Create Reranking Function

```python
def get_result(query,passages,choice):
  if choice == "Nano":
    ranker = Ranker()
  elif choice == "Small":
    ranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="/opt")
  elif choice == "Medium":
    ranker = Ranker(model_name="rank-T5-flan", cache_dir="/opt")
  elif choice == "Large":
    ranker = Ranker(model_name="ms-marco-MultiBERT-L-12", cache_dir="/opt")

  rerankrequest = RerankRequest(query=query, passages=passages)
  results = ranker.rerank(rerankrequest)
  print(results)

  return results
```

### What Happens Internally

1. Model selected based on size
2. Query + passages packed into request
3. Crossâ€‘encoder scores relevance
4. Returns ranked passages

---

# ğŸ§© Step 7 â€” Benchmark Execution Time

```python
%%time
print("sunny")
```

Used to measure execution time inside Colab.

---

## Run Nano Model

```python
%%time
get_result(query,passages,"Nano")
```

Runs fastest reranking model.

---

## Run Small Model

```python
%%time
get_result(query,passages,"Small")
```

Higher ranking precision.

---

## Run Medium Model

```python
%%time
get_result(query,passages,"Medium")
```

Best zeroâ€‘shot performance.

---

# ğŸ§© Step 8 â€” Install LangChain Dependencies

```python
!pip install langchain_community
!pip install langchain_openai
```

Installs LangChain integrations.

---

# ğŸ§© Step 9 â€” Load API Key (Colab Secrets)

```python
from google.colab import userdata
OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')

import os
os.environ["OPENAI_API_KEY"]=OPENAI_API_KEY
```

Loads OpenAI key securely from Colab secrets.

---

# ğŸ§© Step 10 â€” Import LangChain Modules

```python
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
```

### Purpose

| Module       | Role                     |
| ------------ | ------------------------ |
| TextLoader   | Load documents           |
| TextSplitter | Chunk text               |
| Embeddings   | Convert text â†’ vectors   |
| FAISS        | Vector similarity search |

---

# ğŸ§© Step 11 â€” Load Document

```python
documents = TextLoader("/content/state_of_the_union.txt").load()
```

Loads text file into LangChain Document objects.

---

# ğŸ§© Step 12 â€” Chunk Documents

```python
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
texts = text_splitter.split_documents(documents)
```

### Why Chunking?

LLMs and embeddings work better with smaller semantic units.

| Parameter     | Meaning              |
| ------------- | -------------------- |
| chunk_size    | characters per chunk |
| chunk_overlap | shared context       |

---

# ğŸ§© Step 13 â€” Add Metadata IDs

```python
for id, text in enumerate(texts):
    text.metadata["id"] = id
```

Adds unique identifier for tracking retrieved chunks.

---

# ğŸ§© Step 14 â€” Create Embeddings

```python
embedding = OpenAIEmbeddings(model="text-embedding-ada-002")
```

Converts text chunks into dense vectors.

---

# ğŸ§© Step 15 â€” Install FAISS

```python
!pip install faiss-cpu
```

FAISS enables efficient vector similarity search.

---

# ğŸ§© Step 16 â€” Create Retriever

```python
retriever = FAISS.from_documents(texts, embedding).as_retriever(search_kwargs={"k": 10})
```

### Pipeline

Text â†’ Embedding â†’ FAISS Index â†’ Retriever

Returns topâ€‘10 similar chunks.

---

# ğŸ§© Step 17 â€” Query Retrieval

```python
query = "What did the president say about Ketanji Brown Jackson"
docs = retriever.invoke(query)
```

Retriever performs **vector similarity search**.

---

# ğŸ§© Step 18 â€” Inspect Retrieved Docs

```python
pretty_print_docs(docs)
```

Displays retrieved chunks.

---

# ğŸ§© Step 19 â€” Contextual Compression Setup

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import FlashrankRerank
from langchain_openai import ChatOpenAI
```

### Key Idea

Instead of returning all retrieved docs â†’ **compress them using reranking**.

---

# ğŸ§© Step 20 â€” Initialize LLM

```python
llm = ChatOpenAI(temperature=0)
```

Deterministic answer generation.

---

# ğŸ§© Step 21 â€” Create FlashRank Compressor

```python
compressor = FlashrankRerank()
```

Uses FlashRank as document reranker.

---

# ğŸ§© Step 22 â€” Create Compression Retriever

```python
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)
```

### Workflow

1. Retriever fetches candidates
2. FlashRank reranks them
3. Keeps only most relevant passages

---

# ğŸ§© Step 23 â€” Invoke Compression Retriever

```python
compressed_docs = compression_retriever.invoke(
    "What did the president say about Ketanji Jackson Brown"
)
```

Produces filtered highâ€‘relevance documents.

---

# ğŸ§© Step 24 â€” Check Result Size

```python
len(compressed_docs)
```

Shows number of remaining documents after compression.

---

# ğŸ§© Step 25 â€” Inspect Metadata IDs

```python
print([doc.metadata["id"] for doc in compressed_docs])
```

Tracks which chunks survived reranking.

---

# ğŸ§© Step 26 â€” Print Compressed Documents

```python
pretty_print_docs(compressed_docs)
```

Shows reranked context.

---

# ğŸ§© Step 27 â€” Build RetrievalQA Chain

```python
from langchain.chains import RetrievalQA

chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=compression_retriever
)
```

### Architecture

User Query â†’ Retrieval â†’ Reranking â†’ LLM â†’ Answer

---

# ğŸ§© Step 28 â€” Ask Final Question

```python
chain.invoke(query)
```

LLM generates answer using reranked context.

---

# ğŸ§  Full Pipeline Architecture

```
User Query
     â†“
Vector Retriever (FAISS)
     â†“
FlashRank Crossâ€‘Encoder Reranking
     â†“
Context Compression
     â†“
LLM (ChatOpenAI)
     â†“
Final Answer
```

---

# âœ… Why FlashRank Improves RAG

| Without Reranking     | With FlashRank             |
| --------------------- | -------------------------- |
| Topâ€‘k similarity only | Semantic relevance scoring |
| Noisy context         | Clean context              |
| More hallucination    | Reduced hallucination      |
| Token waste           | Efficient tokens           |

---

# ğŸ“Š When to Use FlashRank

* RAG pipelines
* Search engines
* QA systems
* Chatbots
* Lowâ€‘latency APIs
* Serverless deployments

---

# âš™ï¸ Performance Insight

FlashRank uses **Crossâ€‘Encoders**:

```
Score(query, passage) = Transformer([query + passage])
```

Unlike biâ€‘encoders, query and passage interact directly.

Result â†’ higher ranking accuracy.

---

# ğŸ Conclusion

This notebook demonstrates a **productionâ€‘grade RAG enhancement** using FlashRank reranking.

Key Learnings:

* Dense retrieval finds candidates
* Crossâ€‘encoder reranking improves relevance
* Context compression reduces noise
* LLM answers become more accurate

---

# ğŸ“ Requirements

```
flashrank
langchain
langchain_openai
langchain_community
faiss-cpu
```

---

# â­ End of README
