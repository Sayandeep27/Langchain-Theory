# Corrective Retrievalâ€‘Augmented Generation (CRAG)

---

## ğŸ“Œ Overview

**Corrective Retrievalâ€‘Augmented Generation (CRAG)** is an advanced Retrievalâ€‘Augmented Generation (RAG) architecture designed to improve the reliability, accuracy, and factual grounding of Large Language Model (LLM) responses.

Traditional RAG retrieves documents based only on similarity scores and directly sends them to the LLM for generation. However, similarity does **not guarantee correctness**.

CRAG introduces a **knowledge correction layer** that evaluates, filters, refines, and supplements retrieved knowledge before answer generation.

---

## ğŸ¯ Why CRAG is Needed

Below are the key reasons why CRAG improves traditional RAG systems:

| Problem              | Description                                              | How CRAG Solves It                    |
| -------------------- | -------------------------------------------------------- | ------------------------------------- |
| Irrelevant Retrieval | Similar documents may not answer the query               | Evaluates and filters documents       |
| Noise & Errors       | Outdated or lowâ€‘quality information appears in retrieval | Removes noisy content                 |
| Hallucinations       | LLM generates incorrect answers from poor context        | Validates knowledge before generation |
| Reliability          | Critical domains require verified data                   | Adds evaluation and correction steps  |
| Ranking Issues       | Similarity ranking is imperfect                          | Reâ€‘ranks using quality + relevance    |
| Dynamic Knowledge    | Static KB becomes outdated                               | Triggers web search when needed       |
| Bias Reduction       | Retrieval may favor frequent patterns                    | Adds validation beyond similarity     |

---

## ğŸ§  Example Problem

### Query

```
What do koalas eat?
```

### Vanilla RAG Retrieval

| Retrieved Document           | Status       |
| ---------------------------- | ------------ |
| Koalas eat eucalyptus leaves | âœ… Relevant   |
| Pandas eat bamboo            | âŒ Irrelevant |
| Kangaroos graze grass        | âŒ Irrelevant |

Mixed context may confuse the LLM.

### CRAG Solution

CRAG filters irrelevant documents and keeps only validated information.

Final Answer:

```
Koalas primarily eat eucalyptus leaves.
```

---

## ğŸ—ï¸ CRAG Architecture

CRAG divides the pipeline into three main stages:

```
1. Retrieval
2. Knowledge Correction
3. Generation
```

The **Knowledge Correction Layer** is the key innovation.

---

## âš™ï¸ Stepâ€‘byâ€‘Step Working of CRAG

---

### 1ï¸âƒ£ Input Query

The system begins with a user query:

```
X = "What do koalas eat?"
```

---

### 2ï¸âƒ£ Retrieval (Vanilla RAG Step)

The retriever searches the vector database.

```
Topâ€‘K documents â†’ d1, d2, d3...
```

Selection is based only on embedding similarity.

---

### 3ï¸âƒ£ Retrieval Evaluator (Core CRAG Component)

The evaluator checks whether retrieved documents truly answer the query.

#### Evaluation Criteria

* Semantic relevance
* Factual correctness
* Completeness
* Consistency
* Freshness

Each document receives a **relevance score**.

---

### 4ï¸âƒ£ Decision Phase

CRAG classifies retrieval quality into three categories.

#### âœ… Correct

```
At least one document has high relevance.
```

Action:

```
Proceed to Knowledge Refinement
```

---

#### âš ï¸ Ambiguous

```
Medium confidence in retrieved documents.
```

Action:

```
Combine internal + external knowledge
```

---

#### âŒ Incorrect

```
All documents have low relevance.
```

Action:

```
Trigger external web search
```

---

### 5ï¸âƒ£ Corrective Step â€” Knowledge Refinement (If Correct)

Instead of directly sending documents to the LLM, CRAG refines them.

#### (a) Decompose

```
Document â†’ smaller strips/chunks
```

Purpose:

* Fineâ€‘grained filtering
* Remove irrelevant sentences

---

#### (b) Filter

Removes:

* noisy content
* outdated information
* unrelated text

---

#### (c) Reâ€‘rank

Documents are ranked using:

| Factor     | Meaning           |
| ---------- | ----------------- |
| Similarity | Semantic match    |
| Quality    | Factual accuracy  |
| Freshness  | Updated knowledge |
| Coverage   | Completeness      |

---

#### (d) Deduplication

Prevents repeated or duplicated information.

---

#### (e) Recompose

Filtered knowledge becomes:

```
k_in (internal refined knowledge)
```

---

### 6ï¸âƒ£ Web Search (If Incorrect)

CRAG rewrites the query:

```
Original: What do koalas eat?
Rewritten: koala diet eucalyptus leaves wikipedia
```

Then performs web search:

```
k1, k2, k3 â†’ Selected â†’ k_ex
```

Where:

```
k_ex = external knowledge
```

This enables dynamic knowledge retrieval.

---

### 7ï¸âƒ£ Knowledge Combining (If Ambiguous)

CRAG merges both knowledge sources:

```
k_in + k_ex
```

Reason:

* Internal data may be partially correct
* External search fills missing gaps

---

### 8ï¸âƒ£ Answer Generation

The LLM receives only corrected knowledge.

| Decision  | Generator Input |
| --------- | --------------- |
| Correct   | X + k_in        |
| Ambiguous | X + k_in + k_ex |
| Incorrect | X + k_ex        |

Final answer is generated using validated context.

---

## ğŸ”„ Full CRAG Pipeline

```
Query
  â†“
Retrieve Documents
  â†“
Evaluate Retrieval Quality
  â†“
Decision
  â”œâ”€â”€ Correct â†’ Refine Knowledge
  â”œâ”€â”€ Ambiguous â†’ Combine Knowledge
  â””â”€â”€ Incorrect â†’ Web Search
  â†“
Generate Answer
```

---

## âœ… Advantages of CRAG

| Advantage              | Description                         |
| ---------------------- | ----------------------------------- |
| Improved Accuracy      | Filters misleading information      |
| High Reliability       | Suitable for critical domains       |
| Reduced Hallucinations | Validates context before generation |
| Domain Adaptability    | Custom evaluators possible          |
| Better Reasoning       | Cleaner context improves logic      |
| Explainability         | Decisions are traceable             |
| Optimized QA Mapping   | Matches intent, not just similarity |

---

## âš ï¸ Challenges of CRAG

| Challenge            | Description                            |
| -------------------- | -------------------------------------- |
| Complex Architecture | More components than RAG               |
| Scalability Issues   | Higher compute requirements            |
| Domain Dependence    | Correction models may need tuning      |
| High Latency         | Extra evaluation steps                 |
| Overâ€‘Filtering       | Important data may be removed          |
| External Bias Risk   | Web sources may contain misinformation |

---

## ğŸ“Š CRAG vs Traditional RAG

| Feature               | RAG    | CRAG   |
| --------------------- | ------ | ------ |
| Retrieval Validation  | âŒ      | âœ…      |
| Noise Filtering       | âŒ      | âœ…      |
| Dynamic Knowledge     | âŒ      | âœ…      |
| Hallucination Control | Medium | Strong |
| Reliability           | Medium | High   |
| System Complexity     | Low    | High   |

---

## ğŸ§© Oneâ€‘Line Intuition

**RAG:**

```
Retrieve â†’ Generate
```

**CRAG:**

```
Retrieve â†’ Verify â†’ Correct â†’ Generate
```

---

## ğŸ“š Key Takeaway

CRAG enhances traditional RAG by introducing a **selfâ€‘correcting retrieval mechanism** that ensures only reliable, relevant, and validated knowledge reaches the LLM, significantly improving factual accuracy and reducing hallucinations.

---

## â­ Summary

CRAG transforms retrieval from a passive similarity search into an **active qualityâ€‘controlled knowledge pipeline**, making modern AI systems more trustworthy and productionâ€‘ready.

---
